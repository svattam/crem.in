---
layout: post
title: "NIST SRE 2016: Runners-up"
---

Results for the 2016 [NIST Speaker Recognition Evaluation](https://www.nist.gov/itl/iad/mig/speaker-recognition) was announced today. I was part of the MIT team which was placed second in a pool of more than 45 serious research teams competing from around the world. My contribution included developing a backend for calibrating and fusing several different speaker recognition models developed by other researchers on my team. Calibration is a serious issue when there are several different models and we have to combine their individual predictions in order to make an overall prediction. This issue arises because different models have their own quirks - for example, some models tend to predict probabilities conservatively (meaning closer to mid-range), some toward extremes, and others none at all (true conditional probabilities are unknown). If your metric cares about exact probabilities (e.g., logarithmic loss), we need to calibrate the models before fusing their predictions to get an average estimate. This involves a post-processing step where you learn the characteristics of the model behavior. There are two popular methods for calibration: Platt's scaling and Isotonic regression. [Platt's scaling](https://en.wikipedia.org/wiki/Platt_scaling) amounts to training a logistic regression model on the estimator outputs. In [Isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression), the idea is to fit a piecewise-constant monotonically increasing function (e.g., stair shaped function) instead of logistic regression. Alas, I cannot disclose the secret sauce that went into our backend calibrator yet.